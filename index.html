<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Setup-independent Full Projector Compensation</title>
  
  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/index.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700;800&display=swap" rel="stylesheet">
  
  <script defer src="static/js/fontawesome.all.min.js"></script>
</head>
<body>

  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">Setup-independent Full Projector Compensation</h1>
            
            <div class="is-size-5 publication-authors">
              <span class="author-block">
                <a href="https://orcid.org/0009-0006-9201-104X" target="_blank">Haibo Li</a><sup>1</sup>,
              </span>
              <span class="author-block">
                <a href="https://orcid.org/0009-0001-8471-2480" target="_blank">Qingyue Deng</a><sup>1</sup>,
              </span>
              <span class="author-block">
                <a href="https://orcid.org/0009-0003-2823-751X" target="_blank">Jijiang Li</a><sup>1</sup>,
              </span>
              <span class="author-block">
                <a href="https://orcid.org/0000-0003-4094-8413" target="_blank">Haibin Ling</a><sup>2</sup>,
              </span>
              <span class="author-block">
                <a href="https://orcid.org/0000-0002-8647-5730" target="_blank">Bingyao Huang</a><sup>1†</sup>
              </span>
            </div>

            <div class="is-size-5 publication-authors">
              <span class="author-block"><sup>1</sup>Southwest University,</span>
              <span class="author-block"><sup>2</sup>Westlake University</span>
              <span class="eql-cntrb"><small><br><sup>†</sup>Corresponding Author</small></span>
            </div>

            <div class="column has-text-centered">
              <div class="publication-links">
                <span class="link-block">
                  <a href="static/pdfs/paper.pdf" target="_blank" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon"><i class="fas fa-file-pdf"></i></span>
                    <span>Paper</span>
                  </a>
                </span>

                <span class="link-block">
                  <a href="https://github.com/hai-bo-li/SIComp" target="_blank" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon"><i class="fab fa-github"></i></span>
                    <span>Code</span>
                  </a>
                </span>

                <span class="link-block">
                  <a href="https://github.com/hai-bo-li/SIComp" target="_blank" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon"><i class="fas fa-database"></i></span>
                    <span>Dataset</span>
                  </a>
                </span>
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>

  <section class="section hero is-light">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Abstract</h2>
          <div class="content has-text-justified">
            <p>
              Projector compensation seeks to correct geometric and photometric distortions that occur when images are projected onto nonplanar or textured surfaces. However, most existing methods are highly setup-dependent, requiring fine-tuning or retraining whenever the surface, lighting, or projector–camera pose changes. Progress has been limited by two key challenges: (1) the absence of large, diverse training datasets and (2) the entanglement of geometric correction within trained networks. We introduce <strong>SIComp</strong>, the first <strong>S</strong>etup-<strong>I</strong>ndependent framework for full projector <strong>Comp</strong>ensation, capable of generalizing to unseen setups without fine-tuning or retraining. To enable this, we construct a large-scale real-world dataset spanning 277 distinct projector–camera setups. SIComp adopts a co-adaptive design that decouples geometry and photometry: A carefully tailored optical flow module performs online geometric correction, while a novel photometric network handles photometric compensation. To further enhance robustness under varying illumination, we integrate intensity-varying surface priors into the network design. Extensive experiments demonstrate that SIComp consistently produces high-quality compensation across diverse unseen setups, substantially outperforming existing methods in terms of generalization ability and establishing the first generalizable solution to projector compensation.
            </p>
          </div>
        </div>
      </div>
    </div>
  </section>

  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>@article{li2026sicomp,
  title={Setup-independent Full Projector Compensation},
  author={Li, Haibo and Deng, Qingyue and Li, Jijiang and Ling, Haibin and Huang, Bingyao},
  journal={TVCG},
  year={2026}
}</code></pre>
    </div>
  </section>

  <footer class="footer">
    <div class="container">
      <div class="content has-text-centered">
        <p>Built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template">Academic Project Page Template</a>.</p>
      </div>
    </div>
  </footer>

</body>
</html>
